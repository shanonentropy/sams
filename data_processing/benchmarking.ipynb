{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Aug 21 12:52:04 2024\n",
    "\n",
    "@author: zahmed\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "\n",
    "# Start a local Dask cluster with specific configuration\n",
    "#client = Client(n_workers=1, threads_per_worker=2, memory_limit='40GB')\n",
    "\n",
    "# Print dashboard link\n",
    "#print(client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpath1 = 'C:/nv_data/data_holder/LN2_bath/*.csv'\n",
    "fpath2 = 'C:/nv_data/data_holder/LN2_bath_CWL650nm/*.csv'\n",
    "fpath = 'C:/nv_data/data_holder/cooling_to_LN2_second_attempt_CWL_650nm/*.csv'\n",
    "f = glob.glob(fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# dask opt\n",
    "# =============================================================================\n",
    "\n",
    "fnm, time_step = [],[]\n",
    "laser_pow, amplitude, peak_center =[],[],[]\n",
    "width, debye_waller, frame_num = [], [],[]\n",
    "kld, wasserstein_dist =[], []\n",
    "amplitude2, peak_center2, width2 =[],[],[]\n",
    "temps = []\n",
    "\n",
    "filtered_files = f\n",
    "huang_rhys = [649, 780]\n",
    "nv_zpl = [634.25,640.25]\n",
    "nv0_zpl = [572.0, 578]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reference_spectra(filtered_files, ref_index =1):\n",
    "    ''' returns the reference spectra for kl_div computation\n",
    "    it is separated so I can call it once at the start of the computation\n",
    "    loop and not have to reload it with each new spectra'''\n",
    "    df0=pd.read_csv(filtered_files[ref_index], sep=',', header = 0, engine='python')\n",
    "    df0.sort_values(by='Wavelength', ascending=True)\n",
    "    df0.drop_duplicates(subset='Wavelength', keep='first', inplace=True)\n",
    "    spectrum1 = df0['Intensity']/np.sum(df0['Intensity'])\n",
    "    #print('x') # test line to see if this is invoked once or many times\n",
    "    return spectrum1\n",
    "\n",
    "reference_spectra(filtered_files)\n",
    "   \n",
    "def kl_divergence(y, ref):\n",
    "    ''' spectrum1 is the reference file\n",
    "        spectrum2 is the current file in the loop being processed\n",
    "        \n",
    "        need to write the loop here\n",
    "        \n",
    "        '''\n",
    "    #df_=pd.read_csv(spectrum2, sep=',', header = 0, engine='python')\n",
    "    #df_.sort_values(by='Wavelength', ascending=True)\n",
    "    #df_.drop_duplicates(subset='Wavelength', keep='first', inplace=True)\n",
    "    # Normalize the spectra\n",
    "    spectrum = y  / np.sum(y)\n",
    "    \n",
    "    # Calculate the logarithm of the ratio of the two spectra\n",
    "    ratio = np.log(spectrum1 / spectrum)\n",
    "    # Multiply the ratio by the normalized spectra\n",
    "    result = ratio*spectrum1\n",
    "    # Sum the resulting values to obtain the KL divergence\n",
    "    kl_div = np.sum(result)\n",
    "    kld.append(kl_div)\n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "def gaussian(x_zpl, amp, u, std):\n",
    "    ''' gaussian fit'''\n",
    "    return amp*np.exp(-((x_zpl-u)**2/(2*std**2)))\n",
    "\n",
    "\n",
    "def main_processor( nv_type='nv', func = 'gaussian', fit_params = [4000, 637.5,1.5], max_fev=50000, dx = 0.01 ):\n",
    "    ''' nv_type = enter nv for nv(-) or nv0 for nv_zero; default is nv\n",
    "    \n",
    "    func is the fitting function used. default is gaussian, other options \n",
    "    include lorenztian, two_gaussian or two_lorenztian and spline_fit\n",
    "    \n",
    "    fit_parms: default for gaussian\n",
    "    for lorenzian \n",
    "    \n",
    "    '''\n",
    "    if nv_type == 'nv':\n",
    "       zp= nv_zpl\n",
    "    else:\n",
    "        zp=nv0_zpl\n",
    "    ref = reference_spectra(filtered_files)\n",
    "    \n",
    "    for f1 in filtered_files[:]:\n",
    "        #print(f1)\n",
    "        #fnm.append(f1) #.split('\\\\')[1])\n",
    "        #frame_num.append((f1))\n",
    "        ###### open and clean data ####\n",
    "        df=pd.read_csv(f1, sep=',', header = 0, engine='python')\n",
    "        df.sort_values(by='Wavelength', ascending=True)\n",
    "        df.drop_duplicates(subset='Wavelength', keep='first', inplace=True)\n",
    "        x,y=df['Wavelength'],df['Intensity']\n",
    "        ### mark out zpl range of interest #####\n",
    "        x_zpl, y_zpl = x[(np.abs(x-zp[0])).argmin():(np.abs(x-zp[1])).argmin() ],\\\n",
    "        y[(np.abs(x-zp[0])).argmin():(np.abs(x-zp[1])).argmin() ]\n",
    "\n",
    "        base = peakutils.baseline(y_zpl, 1)\n",
    "        y_zpl_base = y_zpl-base\n",
    "        #time_step.append(time_st(f1))\n",
    "        #kl_divergence(y, ref)\n",
    "        #wasserstein_dist.append(wasserstein_distance(y,spectrum1))\n",
    "        dx_val = (x[0]-x[50])/50\n",
    "        area_zpl = trapz(y[(np.abs(x-zp[0])).argmin():(np.abs(x-zp[1])).argmin() ], dx= dx_val)\n",
    "        area_psb = trapz(y[(np.abs(x-huang_rhys[0])).argmin():(np.abs(x-huang_rhys[1])).argmin() ], dx= dx_val)\n",
    "        dw = area_zpl/area_psb\n",
    "        debye_waller.append(dw); \n",
    "        if func == 'gaussian': \n",
    "             popt, pcov = curve_fit(gaussian,x_zpl, y_zpl_base, [4000, 637.5,1.5], maxfev=max_fev )\n",
    "             amp, center_wavelength, FWHM = popt\n",
    "             peak_center.append(center_wavelength);\n",
    "             width.append(FWHM);\n",
    "             amplitude.append(amp);\n",
    "             #lp = laser_power(f1)\n",
    "             #laser_pow.append(lp)\n",
    "             #temps.append(float(temp(f1)))\n",
    "\n",
    "\n",
    "a =time.time()\n",
    "main_processor()\n",
    "b = time.time() - a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# dask\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "from dask.distributed import Client, progress\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.integrate import trapz\n",
    "import peakutils\n",
    "import dask\n",
    "\n",
    "# Initialize Dask client with dashboard\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='40GB', dashboard_address=':8787')\n",
    "\n",
    "def process_file(f1, nv_type='nv', func='gaussian', fit_params=[4000, 637.5, 1.5], max_fev=50000, dx=0.01):\n",
    "    ''' Process a single file and extract features '''\n",
    "    \n",
    "    if nv_type == 'nv':\n",
    "        zp = nv_zpl\n",
    "    else:\n",
    "        zp = nv0_zpl\n",
    "    \n",
    "    df = pd.read_csv(f1, sep=',', header=0, engine='python')#.compute()\n",
    "    df = df.sort_values(by='Wavelength', ascending=True).drop_duplicates(subset='Wavelength', keep='first')\n",
    "    x, y = df['Wavelength'], df['Intensity']\n",
    "    \n",
    "    # Mark out ZPL range of interest\n",
    "    x_zpl_range = (np.abs(x - zp[0])).argmin(), (np.abs(x - zp[1])).argmin()\n",
    "    x_zpl, y_zpl = x[x_zpl_range[0]:x_zpl_range[1]], y[x_zpl_range[0]:x_zpl_range[1]]\n",
    "    \n",
    "    base = peakutils.baseline(y_zpl, 1)\n",
    "    y_zpl_base = y_zpl - base\n",
    "    \n",
    "    dx_val = (x[0] - x[50]) / 50\n",
    "    area_zpl = trapz(y[x_zpl_range[0]:x_zpl_range[1]], dx=dx_val)\n",
    "    area_psb = trapz(y[(np.abs(x - huang_rhys[0])).argmin():(np.abs(x - huang_rhys[1])).argmin()], dx=dx_val)\n",
    "    dw = area_zpl / area_psb\n",
    "    \n",
    "    result = {'debye_waller': dw}\n",
    "    \n",
    "    if func == 'gaussian':\n",
    "        def gaussian(x_zpl, amp, u, std):\n",
    "            return amp * np.exp(-((x_zpl - u) ** 2 / (2 * std ** 2)))\n",
    "        \n",
    "        popt, _ = curve_fit(gaussian, x_zpl, y_zpl_base, p0=fit_params, maxfev=max_fev)\n",
    "        amp, center_wavelength, FWHM = popt\n",
    "        \n",
    "        result.update({'amplitude': amp, 'peak_center': center_wavelength, 'width': FWHM})\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of files to process\n",
    "files = filtered_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a =time.time()\n",
    "main_processor()\n",
    "b = time.time() - a\n",
    "\n",
    "\n",
    "\n",
    "c =time.time()\n",
    "# Create and compute delayed tasks\n",
    "delayed_results = [delayed(process_file)(f) for f in files]\n",
    "results = dask.compute(*delayed_results)\n",
    "\n",
    "# Monitor task progress\n",
    "progress(delayed_results)\n",
    "\n",
    "# Compute results\n",
    "\n",
    "#results = dask.compute(*delayed_results)\n",
    "\n",
    "\n",
    "d = time.time() - c\n",
    "\n",
    "\n",
    "\n",
    "print(b, d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
